{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import functions \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import glob\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pylab import MaxNLocator\n",
    "import rioxarray\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geo_idx(dd, dd_array):\n",
    "   \"\"\"\n",
    "     search for nearest decimal degree in an array of decimal degrees and return the index.\n",
    "     np.argmin returns the indices of minium value along an axis.\n",
    "     so subtract dd from all values in dd_array, take absolute value and find index of minium.\n",
    "    \"\"\"\n",
    "   geo_idx = (np.abs(dd_array - dd)).argmin()\n",
    "   return geo_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from inspect import signature\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "# @profile\n",
    "def parallel(function, it, nbrCores, noInteract=False):\n",
    "    #evaluates in parallel all the results, not lazy\n",
    "\n",
    "    #disable interactive window for plots, also kills existing figures\n",
    "    if noInteract: plt.switch_backend('Agg')\n",
    "\n",
    "    #generate pool of workers\n",
    "    with Pool(processes = nbrCores) as pool:\n",
    "\n",
    "        #single argument for function\n",
    "        if len(signature(function).parameters)==1:\n",
    "            results = pool.map(function, it)\n",
    "\n",
    "        #mutliple arguments for function\n",
    "        else:\n",
    "            results = pool.starmap(function, it)\n",
    "\n",
    "    #kill all processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    #re enable interactive window for plots\n",
    "    if noInteract: \n",
    "        try: plt.switch_backend('Qt5Agg')\n",
    "        except ImportError: pass   #pass if non-interactive shell \n",
    "\n",
    "    if type(results[0])==tuple:\n",
    "       \n",
    "        return (np.asarray(r) for r in zip(*results)) \n",
    "    #convert directly to numpy array, merge each proc times\n",
    "    else:\n",
    "        return np.asarray(results)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "def distrib_task(begin, end, division) :\n",
    "\n",
    "    #job distribution\n",
    "    size   = end - begin + 1    #total number of indexes\n",
    "    segm   = size // division   #number of iteration to perform per division\n",
    "    remain = size % division    #remaining indexes to calculate after division\n",
    "\n",
    "    #handles case if less time elements than division\n",
    "    if segm == 0: division = remain\n",
    "\n",
    "    #initialization\n",
    "    lower = begin\n",
    "    jobs = [None]*division\n",
    "\n",
    "    #loop over divisions\n",
    "    for i in range(division):\n",
    "\n",
    "        #distribute indexes into the divisions, accounting for remaining loops\n",
    "        upper = lower + segm    #upper index\n",
    "        if remain > 0:\n",
    "            upper  += 1      #added +1 if additional index to process\n",
    "            remain -= 1      #keep track of the remaining loops to distribute\n",
    "\n",
    "        jobs[i] = slice(lower,upper,1)\n",
    "        lower = upper             #next pair lower index\n",
    "        #next division\n",
    "\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rich import print\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from xlsx2csv import Xlsx2csv\n",
    "from io import StringIO\n",
    "\n",
    "def timer(name, startTime = None):\n",
    "    if startTime:\n",
    "        print(f\"Timer: Elapsed time for [{name}]: {datetime.now() - startTime}\")\n",
    "    else:\n",
    "        startTime = datetime.now()\n",
    "        print(f\"Timer: Starting [{name}] at {startTime}\")\n",
    "        return startTime\n",
    "\n",
    "\n",
    "def read_excel(path: str, sheet_index: int) -> pd.DataFrame:\n",
    "    print(path)\n",
    "    buffer = StringIO()\n",
    "    Xlsx2csv(path, outputencoding=\"utf-8\").convert(buffer,sheetid=sheet_index)\n",
    "    buffer.seek(0)\n",
    "    df = pd.read_csv(buffer,low_memory=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def natural_keys_urban(text):\n",
    "    start = text.split('w_')[-1].index('t')\n",
    "    end  = text.split('w_')[-1].index('_')\n",
    "    mo = text.split('w_')[-1][start+1:end]\n",
    "    year = text.split('w_')[-1][-4]\n",
    "    return int(year)*12+int(mo)\n",
    "def natural_keys(text):\n",
    "    start = text.split('_y')[-1].index('t')\n",
    "    end  = text.split('_y')[-1].index('/')\n",
    "    mo = text.split('_y')[-1][start+1:end]\n",
    "    year = text.split('_y')[-1][0]\n",
    "    return int(year)*12+int(mo)\n",
    "def natural_keys_ag(text):\n",
    "    start = text.split('w_')[-1].index('m')\n",
    "    end  = text.split('w_')[-1].index('_')\n",
    "    mo = text.split('w_')[-1][start+1:end]\n",
    "    year = text.split('w_')[-1][-4]\n",
    "    return int(year)*12+int(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file_path = r\"/oak/stanford/groups/gorelick/bhima_08262024/modules/hydro/sixyears_bau4year/hydro_outputs\"\n",
    "basepath = r\"/scratch/users/awan005/bhima_mas_code_publication\"\n",
    "file_path = basepath + r\"/outputs/data_raw/sixyears_bau4year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim = ['scenario1_donothing', 'scenario2_donothing'] \n",
    "# scenario1: moderate cliamte change, high urbanization\n",
    "# scenario2: extreme climate change, high urbanization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reservoir storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## reservoir location\n",
    "waterbody = pd.read_csv(basepath + r\"/modules/hydro/hydro_files/excel/Copy of Waterbodies.csv\")\n",
    "waterbody_water = waterbody[waterbody['lat']>0]\n",
    "Reservoirwater = xr.open_dataarray(file_path  + \"_\" + sim[0] + \"/\" +'final_y1_t1' +'/'+ \"lakeResStorage_daily.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Res = {}\n",
    "for i in range(waterbody_water.shape[0]):\n",
    "    Res[waterbody_water['Lake_name'][waterbody_water.index[i]]]=(waterbody_water['lat'][waterbody_water.index[i]],waterbody_water['long'][waterbody_water.index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lats = Reservoirwater['lat'][:]\n",
    "lons = Reservoirwater['lon'][:]\n",
    "lat_idx={}\n",
    "lon_idx={}\n",
    "\n",
    "for keys in Res:\n",
    "    outlet = Res[keys]\n",
    "\n",
    "    in_lat = outlet[0]\n",
    "    in_lon = outlet[1]\n",
    "\n",
    "    lat_idx[keys] = geo_idx(in_lat, lats)\n",
    "    lon_idx[keys] = geo_idx(in_lon, lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "List = ['Bhama Askhed', 'Andra', 'Pawana', 'Mulshi', 'Temghar', 'Warasgaon', 'Panshet', 'Khadakwasla','Ujjani','Veer','Bhatghar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reservoir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_path_tt = []\n",
    "\n",
    "for k in range(len(sim)):\n",
    "    res_path = glob.glob(file_path + '_' + sim[k] + r\"/final_y*/lakeResStorage_daily.nc\")\n",
    "    res_path.sort(key=natural_keys)\n",
    "\n",
    "    res_path_tt+=res_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_storage(k, m):\n",
    "\n",
    "    da_combine_res = xr.open_dataarray(res_path_tt[k])[:,lat_idx[m],lon_idx[m]]\n",
    "\n",
    "    res_tt = da_combine_res[:-1].groupby('time').sum(...).mean().values * 1e-6  \n",
    "\n",
    "    return res_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m256\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "nbrCores= psutil.cpu_count(logical=False)\n",
    "print(str(nbrCores))\n",
    "\n",
    "res_tt2 = {}\n",
    "for i in range(len(List)):\n",
    "\n",
    "    it = ((int(j), List[i]) for j in range(len(res_path_tt)))\n",
    "\n",
    "    res_tt2[List[i]] = parallel(res_storage,it,nbrCores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(res_tt2).to_csv(basepath + r\"/outputs/data_processed/resource_status_metrics/res_forpaper_extremeclimate_ssp1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Khamgoan = (18.545,74.21861111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geo_idx(dd, dd_array):\n",
    "   \"\"\"\n",
    "     search for nearest decimal degree in an array of decimal degrees and return the index.\n",
    "     np.argmin returns the indices of minium value along an axis.\n",
    "     so subtract dd from all values in dd_array, take absolute value and find index of minium.\n",
    "    \"\"\"\n",
    "   geo_idx = (np.abs(dd_array - dd)).argmin()\n",
    "   return geo_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da = Reservoirwater\n",
    "outlet = Khamgoan\n",
    "\n",
    "lats = da['lat'][:]\n",
    "lons = da['lon'][:]\n",
    "\n",
    "in_lat = outlet[0]\n",
    "in_lon = outlet[1]\n",
    "\n",
    "lat_idx = geo_idx(in_lat, lats)\n",
    "lon_idx = geo_idx(in_lon, lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discharge_path_tt = []\n",
    "\n",
    "for k in range(len(sim)):\n",
    "    \n",
    "    discharge_path = glob.glob(file_path + '_' + sim[k] + r\"/final_y*/discharge_daily.nc\")\n",
    "\n",
    "    discharge_path.sort(key=natural_keys)\n",
    "\n",
    "    discharge_path_tt+=discharge_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discharge_river(k, m, n):\n",
    "\n",
    "    da_combine_res = xr.open_dataarray(discharge_path_tt[k])[:,m,n]\n",
    "\n",
    "    res_tt = da_combine_res[:-1].values \n",
    "\n",
    "    return res_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m256\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/awan005/.local/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "nbrCores= psutil.cpu_count(logical=False)\n",
    "print(str(nbrCores))\n",
    "\n",
    "river_tt2 = []\n",
    "\n",
    "\n",
    "it = ((int(j), lat_idx, lon_idx) for j in range(len(discharge_path_tt)))\n",
    "\n",
    "river_tt2 = parallel(discharge_river,it,nbrCores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.hstack(river_tt2)).to_csv(basepath + r\"/outputs/data_processed/resource_status_metrics/river_forpaper_extremeclimate_ssp1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groundwater depletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gw_path_tt = []\n",
    "\n",
    "for k in range(0,len(sim),1):\n",
    "    gw_path = glob.glob(file_path + '_' + sim[k] + r\"/final_y*/gwdepth_adjusted_daily.nc\")\n",
    "\n",
    "    gw_path.sort(key=natural_keys)\n",
    "\n",
    "    gw_path_tt+=gw_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pune_shp = gpd.read_file(basepath + r\"/modules/hydro/hydro_files/shapefiles/PMC_Boundary/PMC_PCMC_Ward_Bounda_Outer.shp\")\n",
    "tanker_shp = gpd.read_file(basepath + r\"/modules/hydro/hydro_files/shapefiles/tanker/tanker.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gw_depletion_basin(s):\n",
    "    \n",
    "    pp_mean = []\n",
    "    tanker_mean = []\n",
    "\n",
    "    gw_path = gw_path_tt[s]\n",
    "    gw_nc = rioxarray.open_rasterio(gw_path,masked=True)[:-1].mean(dim=['time'])\n",
    "    gw_nc.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=True)\n",
    "    gw_nc.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "    \n",
    "    pune_gpd = gw_nc.rio.clip(Pune_shp.geometry.apply(mapping), Pune_shp.crs)    \n",
    "    pp_mean.append(pune_gpd.mean().values)\n",
    "    \n",
    "    tanker_gpd = gw_nc.rio.clip(tanker_shp.geometry.apply(mapping), tanker_shp.crs)    \n",
    "    tanker_mean.append(tanker_gpd.mean().values)\n",
    "\n",
    "    return pp_mean, tanker_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m256\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "nbrCores= psutil.cpu_count(logical=False)\n",
    "print(str(nbrCores))\n",
    "\n",
    "it = ((j) for j in range(len(gw_path_tt)))\n",
    "\n",
    "pp_bb, pp_tanker = parallel(gw_depletion_basin,it,nbrCores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.hstack(pp_bb)).to_csv(basepath + r\"/outputs/data_processed/resource_status_metrics/gw_forpaper_pune_extremeclimate_ssp1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.hstack(pp_tanker)).to_csv(basepath + r\"/outputs/data_processed/resource_status_metrics/gw_forpaper_tanker_extremeclimate_ssp1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundwater map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xr.open_dataset(gw_path_tt[4+48]).to_netcdf(basepath + r\"/outputs/data_processed/resource_status_metrics/gwdepth_adjusted_daily_avg2.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xr.open_dataset(gw_path_tt[4+48+12]).to_netcdf(basepath + r\"/outputs/data_processed/resource_status_metrics/gwdepth_adjusted_daily_yr1.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xr.open_dataset(gw_path_tt[4+48+24]).to_netcdf(basepath + r\"/outputs/data_processed/resource_status_metrics/gwdepth_adjusted_daily_yr2.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xr.open_dataset(gw_path_tt[4+48+36]).to_netcdf(basepath + r\"/outputs/data_processed/resource_status_metrics/gwdepth_adjusted_daily_yr3.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
